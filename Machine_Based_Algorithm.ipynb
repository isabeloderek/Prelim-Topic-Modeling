{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTcykHs7NgWe",
    "outputId": "37902301-840a-4f32-a1d3-095e297474ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/andrianllmm/tagalog-stemmer.git@main\n",
      "  Cloning https://github.com/andrianllmm/tagalog-stemmer.git (to revision main) to c:\\users\\my pc\\appdata\\local\\temp\\pip-req-build-761b9fmo\n",
      "  Resolved https://github.com/andrianllmm/tagalog-stemmer.git to commit b5babfd4caebf8a8f480f8adab9f1c97f42a3baa\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from tglstemmer==0.0.1) (3.8.1)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from tglstemmer==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: click in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from click->nltk>=3.8.1->tglstemmer==0.0.1) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\my pc\\appdata\\roaming\\python\\python311\\site-packages\\fonttools-4.51.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\my pc\\appdata\\roaming\\python\\python311\\site-packages\\gluoncv-0.11.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/andrianllmm/tagalog-stemmer.git 'C:\\Users\\MY PC\\AppData\\Local\\Temp\\pip-req-build-761b9fmo'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\my pc\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\my pc\\appdata\\roaming\\python\\python311\\site-packages\\fonttools-4.51.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\my pc\\appdata\\roaming\\python\\python311\\site-packages\\gluoncv-0.11.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.9.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\my pc\\appdata\\roaming\\python\\python311\\site-packages\\fonttools-4.51.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\my pc\\appdata\\roaming\\python\\python311\\site-packages\\gluoncv-0.11.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "# This command installs a Python package directly from a GitHub repository.\n",
    "# It uses pip to install the Tagalog Stemmer package from the repository hosted at the given URL.\n",
    "!pip install git+https://github.com/andrianllmm/tagalog-stemmer.git@main\n",
    "!pip install langdetect\n",
    "!pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEXxvYxhNl1I",
    "outputId": "5c281216-3976-4026-c796-ebc514a5d6b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MY\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\MY\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\MY\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "\n",
    "# Importing visualization libraries\n",
    "import matplotlib.pyplot as plt  # For plotting and data visualization\n",
    "from matplotlib import style  # For setting plot styles\n",
    "import seaborn as sns  # For advanced data visualization (heatmaps, categorical plots)\n",
    "\n",
    "# Importing Natural Language Toolkit (nltk) for NLP operations\n",
    "import nltk\n",
    "nltk.download('punkt')  # Downloading 'punkt' tokenizer for sentence and word tokenization\n",
    "\n",
    "# Importing functions from nltk for stopwords and tokenization\n",
    "from nltk.corpus import stopwords  # For using stopwords (commonly removed words like 'and', 'the')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Functions for word and sentence tokenization\n",
    "\n",
    "# Importing stemmers (for reducing words to their root form)\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer  # Porter and Lancaster stemming algorithms\n",
    "\n",
    "# Importing WordNet lemmatizer (for converting words to their base form using linguistic rules)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Importing Tagalog stemmer from the 'tglstemmer' package (assumed installed in earlier command)\n",
    "from tglstemmer import stemmer  # Tagalog language stemmer\n",
    "\n",
    "# Importing vectorizers from scikit-learn for creating document-term matrices (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # TF-IDF and Count vectorization\n",
    "\n",
    "#Downloading 'stopwords'\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Defining a set of stop words for use in text processing\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))  # Stop words in English\n",
    "\n",
    "# Downloading 'wordnet' corpus for lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Importing regular expression library for text manipulation and pattern matching\n",
    "import re  # For regex operations such as cleaning text data\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eU5LenrTNp3y",
    "outputId": "e509f6e4-4dec-4389-a3c8-4d24e1e1f208"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ako',\n",
       " 'sa',\n",
       " 'akin',\n",
       " 'ko',\n",
       " 'aking',\n",
       " 'sarili',\n",
       " 'kami',\n",
       " 'atin',\n",
       " 'ang',\n",
       " 'aming',\n",
       " 'amin',\n",
       " 'ating',\n",
       " 'ka',\n",
       " 'iyong',\n",
       " 'iyo',\n",
       " 'inyong',\n",
       " 'siya',\n",
       " 'kanya',\n",
       " 'mismo',\n",
       " 'ito',\n",
       " 'nito',\n",
       " 'kanyang',\n",
       " 'sila',\n",
       " 'nila',\n",
       " 'kanila',\n",
       " 'kanilang',\n",
       " 'kung',\n",
       " 'ano',\n",
       " 'alin',\n",
       " 'sino',\n",
       " 'kanino',\n",
       " 'na',\n",
       " 'mga',\n",
       " 'iyon',\n",
       " 'am',\n",
       " 'ay',\n",
       " 'maging',\n",
       " 'naging',\n",
       " 'mayroon',\n",
       " 'may',\n",
       " 'nagkaroon',\n",
       " 'pagkakaroon',\n",
       " 'gumawa',\n",
       " 'ginagawa',\n",
       " 'ginawa',\n",
       " 'paggawa',\n",
       " 'ibig',\n",
       " 'dapat',\n",
       " 'maaari',\n",
       " 'marapat',\n",
       " 'kong',\n",
       " 'ikaw',\n",
       " 'tayo',\n",
       " 'hindi',\n",
       " 'namin',\n",
       " 'gusto',\n",
       " 'nais',\n",
       " 'niyang',\n",
       " 'nilang',\n",
       " 'niya',\n",
       " 'huwag',\n",
       " 'ginawang',\n",
       " 'gagawin',\n",
       " 'maaaring',\n",
       " 'sabihin',\n",
       " 'narito',\n",
       " 'kapag',\n",
       " 'ni',\n",
       " 'nasaan',\n",
       " 'bakit',\n",
       " 'paano',\n",
       " 'kailangan',\n",
       " 'walang',\n",
       " 'katiyakan',\n",
       " 'isang',\n",
       " 'at',\n",
       " 'pero',\n",
       " 'o',\n",
       " 'dahil',\n",
       " 'bilang',\n",
       " 'hanggang',\n",
       " 'habang',\n",
       " 'ng',\n",
       " 'pamamagitan',\n",
       " 'para',\n",
       " 'tungkol',\n",
       " 'laban',\n",
       " 'pagitan',\n",
       " 'panahon',\n",
       " 'bago',\n",
       " 'pagkatapos',\n",
       " 'itaas',\n",
       " 'ibaba',\n",
       " 'mula',\n",
       " 'pataas',\n",
       " 'pababa',\n",
       " 'palabas',\n",
       " 'ibabaw',\n",
       " 'ilalim',\n",
       " 'muli',\n",
       " 'pa',\n",
       " 'minsan',\n",
       " 'dito',\n",
       " 'doon',\n",
       " 'saan',\n",
       " 'lahat',\n",
       " 'anumang',\n",
       " 'kapwa',\n",
       " 'bawat',\n",
       " 'ilan',\n",
       " 'karamihan',\n",
       " 'iba',\n",
       " 'tulad',\n",
       " 'lamang',\n",
       " 'pareho',\n",
       " 'kaya',\n",
       " 'kaysa',\n",
       " 'masyado',\n",
       " 'napaka',\n",
       " 'isa',\n",
       " 'bababa',\n",
       " 'kulang',\n",
       " 'marami',\n",
       " 'ngayon',\n",
       " 'kailanman',\n",
       " 'sabi',\n",
       " 'nabanggit',\n",
       " 'din',\n",
       " 'kumuha',\n",
       " 'pumunta',\n",
       " 'pumupunta',\n",
       " 'ilagay',\n",
       " 'makita',\n",
       " 'nakita',\n",
       " 'katulad',\n",
       " 'mahusay',\n",
       " 'likod',\n",
       " 'kahit',\n",
       " 'paraan',\n",
       " 'noon',\n",
       " 'gayunman',\n",
       " 'dalawa',\n",
       " 'tatlo',\n",
       " 'apat',\n",
       " 'lima',\n",
       " 'una',\n",
       " 'pangalawa',\n",
       " 'yung',\n",
       " 'mo',\n",
       " 'lang',\n",
       " 'mag',\n",
       " 'ba',\n",
       " 'pag',\n",
       " 'yan',\n",
       " 'nga',\n",
       " 'rin',\n",
       " 'kasi',\n",
       " 'po',\n",
       " 'ung']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Tagalog stopwords from a text file\n",
    "# 'tagalog_stop_words.txt' is assumed to be a file containing a list of stopwords in Tagalog.\n",
    "# The file is read as a DataFrame using pandas.\n",
    "tagalog_stopwords = pd.read_csv(\"tagalog_stop_words.txt\")\n",
    "\n",
    "# Extracting the 'stopwords' column from the DataFrame and converting it to a Python list\n",
    "# Assuming the file has a column named 'stopwords' that contains the actual stop words.\n",
    "tagalog_stopwords = tagalog_stopwords['stopwords'].tolist()\n",
    "\n",
    "# Output the list of Tagalog stopwords\n",
    "tagalog_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zAniA4AxNsTU"
   },
   "outputs": [],
   "source": [
    "# Combining English and Tagalog stopwords into a single list\n",
    "# The stopwords from nltk (in English) are combined with the Tagalog stopwords previously loaded from a file.\n",
    "# This allows you to remove both English and Tagalog stopwords from text in one step.\n",
    "all_stopwords = stopwords.words('english') + tagalog_stopwords\n",
    "\n",
    "# The resulting 'all_stopwords' will contain stopwords from both languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "m_GDFMmiNtCt"
   },
   "outputs": [],
   "source": [
    "# Reading the English dataset from a CSV file\n",
    "# 'AI in edu dataset - Sheet1.csv' is assumed to contain text data in English.\n",
    "# The dataset is loaded as a pandas DataFrame.\n",
    "dataset = pd.read_csv('AI in edu dataset - Sheet1.csv')\n",
    "\n",
    "dataset = dataset.drop(dataset[dataset['Sentiment'] == 'Neutral' ].index)\n",
    "dataset = dataset.drop(dataset[dataset['Sentiment'] == 'neutral' ].index)\n",
    "\n",
    "dataset.dropna(subset = ['Sentiment'], inplace=True)\n",
    "\n",
    "# Converting all data in the dataset to string type\n",
    "# Ensures that all values in the DataFrame are treated as text, regardless of their original format.\n",
    "dataset = dataset.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "33ZGjnjM3bR0"
   },
   "outputs": [],
   "source": [
    "#Splitting of dataset\n",
    "X_data, X_unseen, y_data, y_unseen = train_test_split(dataset['Content'], dataset['Sentiment'], test_size=0.1, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lERxEinPNvJa",
    "outputId": "d6a88322-13b3-456e-81cb-b02f3abafe2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\MY\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# Function to replace words with synonyms\n",
    "def synonym_replacement(sentence, n=2):\n",
    "    words = sentence.split()\n",
    "    new_sentence = words.copy()\n",
    "\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))  # Only consider words that have synonyms\n",
    "\n",
    "    # Randomly select 'n' words to replace with their synonyms\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if len(synonyms) > 0:\n",
    "            synonym = synonyms[0].lemmas()[0].name()  # Choose the first synonym\n",
    "            new_sentence = [synonym if word == random_word else word for word in new_sentence]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    print('Done SR')\n",
    "    return ' '.join(new_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "38ggsk1HNyvK"
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Function for back-translation\n",
    "def back_translate(sentence, lang='fr'):\n",
    "    translator = Translator()\n",
    "\n",
    "    # Translate the sentence to the target language\n",
    "    translated = translator.translate(sentence, src='en', dest=lang).text\n",
    "\n",
    "    # Translate it back to English\n",
    "    back_translated = translator.translate(translated, src=lang, dest='en').text\n",
    "\n",
    "    print('Done Translate')\n",
    "    return back_translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MMUOj8ZN3yL",
    "outputId": "0b2693bc-c940-467c-d04e-cadb25d2f4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done SR\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n",
      "Done Translate\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the original dataset\n",
    "augmented_dataset = pd.concat([pd.DataFrame(X_train), y_train], axis=1)\n",
    "\n",
    "# Apply synonym replacement to each row and append to the 'Content' column\n",
    "augmented_dataset['Content_Augmented_Synonym'] = augmented_dataset['Content'].apply(lambda x: synonym_replacement(x, n=1))\n",
    "\n",
    "# Apply back-translation and append to the 'Content' column\n",
    "augmented_dataset['Content_Augmented_BackTranslate'] = augmented_dataset['Content'].apply(lambda x: back_translate(x, lang='fr'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "xpZ9lx89N4VF",
    "outputId": "43e6118e-a76a-471e-d73a-428a2561fe13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MY PC\\AppData\\Local\\Temp\\ipykernel_2016\\3944599926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.rename(columns = {'Content_Augmented_Synonym':'Content'}, inplace = True)\n",
      "C:\\Users\\MY PC\\AppData\\Local\\Temp\\ipykernel_2016\\3944599926.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2.rename(columns = {'Content_Augmented_BackTranslate':'Content'}, inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>data collection by AI tools raises student pri...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>AI EDUCATION volition become the norm. AI teac...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>AI role in education is increasing GlobalData ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Kaya mas mganda tlaga pag I.T., Comsci at iba ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>I used to be a student, and now I am a teacher...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>marami na ang hindi nag aaral at chatgpt na la...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>its ok to use ai just as long as you unserstan...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>For me, malaking tulong ang AI sa pag aaral ko...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Malapit ng mawalan ng trabaho si teacher yun t...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>ayaw kasi mag adapt ng mga teacher. gusto kung...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1079 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Content Sentiment Platform\n",
       "77   data collection by AI tools raises student pri...  Negative      NaN\n",
       "204  AI EDUCATION volition become the norm. AI teac...  Positive      NaN\n",
       "218  AI role in education is increasing GlobalData ...  Positive      NaN\n",
       "268  Kaya mas mganda tlaga pag I.T., Comsci at iba ...  Positive      NaN\n",
       "161  I used to be a student, and now I am a teacher...  Positive      NaN\n",
       "..                                                 ...       ...      ...\n",
       "501  marami na ang hindi nag aaral at chatgpt na la...  Negative  youtube\n",
       "503  its ok to use ai just as long as you unserstan...  Positive  youtube\n",
       "504  For me, malaking tulong ang AI sa pag aaral ko...  Positive  youtube\n",
       "506  Malapit ng mawalan ng trabaho si teacher yun t...  Negative  youtube\n",
       "508  ayaw kasi mag adapt ng mga teacher. gusto kung...  Negative  youtube\n",
       "\n",
       "[1079 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = augmented_dataset[['Content_Augmented_Synonym','Sentiment']]\n",
    "\n",
    "df2 = augmented_dataset[['Content_Augmented_BackTranslate','Sentiment']]\n",
    "df1.rename(columns = {'Content_Augmented_Synonym':'Content'}, inplace = True)\n",
    "df2.rename(columns = {'Content_Augmented_BackTranslate':'Content'}, inplace = True)\n",
    "\n",
    "dataset = pd.concat([df1, df2, dataset])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vj0TKmV8N-nj"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('augmented.csv',index=0) ##WE WILL SAVE AUGMENTED DATASET IN OUR LOCAL STORAGE TO MAKE REPLICATION OR TESTING EASIER IN THE FUTURE SO THAT AUGMENTATION WILL NOT BE NEEDED TO RUN AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "JHHAzNx4OYmT",
    "outputId": "37c12c9e-3d87-4adf-c2a9-a519cbc5b8a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content      1079\n",
       "Sentiment    1079\n",
       "Platform      443\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hb64R8aTOb0W"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    #Determine the language of the sentence\n",
    "    lang = detect(text)\n",
    "\n",
    "\n",
    "    # Remove any character that is not a letter or whitespace using regex\n",
    "    # This step removes punctuation, numbers, and special characters, leaving only letters and spaces.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Initialize the WordNet Lemmatizer (for reducing words to their base form)\n",
    "    le = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the text into individual words\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize each word token and remove stopwords (words in 'all_stopwords') and tokens shorter than 3 characters\n",
    "    tokens = [le.lemmatize(w) for w in word_tokens if w not in all_stopwords and len(w) > 3]\n",
    "\n",
    "    # Stemming based on language\n",
    "    if lang == 'en':\n",
    "        # For other languages (assumed to be English), use the PorterStemmer for stemming\n",
    "        ps = PorterStemmer()\n",
    "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "\n",
    "    else:\n",
    "        # If the language is English, use the Tagalog stemmer ('stemmer' assumed to be the English stemmer)\n",
    "        stemmed_tokens = [stemmer.get_stem(token) for token in tokens]\n",
    "\n",
    "    # Join the processed tokens back into a single string\n",
    "    cleaned_text = \" \".join(stemmed_tokens)\n",
    "\n",
    "    # Return the cleaned and processed text\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mPu-h5bgOclE"
   },
   "outputs": [],
   "source": [
    "# Applying the 'clean_text' function to the 'Content' column of the dataset\n",
    "# For each row in the 'Content' column, it calls the 'clean_text' function, specifying \"english\" as the language.\n",
    "dataset['Content'] = dataset['Content'].apply(lambda x: clean_text(x))\n",
    "X_test = X_test.apply(lambda x: clean_text(x))\n",
    "X_unseen = X_unseen.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pptMUQ8cOugT"
   },
   "outputs": [],
   "source": [
    "# Initializing the TfidfVectorizer\n",
    "# This vectorizer converts the text into a matrix of TF-IDF features.\n",
    "# 'stop_words=all_stopwords' specifies that the TF-IDF vectorizer should ignore both English and Tagalog stopwords.\n",
    "# 'max_features=1000' limits the number of features (terms) to 1000, keeping only the most important ones based on TF-IDF scores.\n",
    "vect = TfidfVectorizer(stop_words=all_stopwords, max_features=415)\n",
    "\n",
    "# Applying the vectorizer to the 'Content' column of the English dataset\n",
    "# The fit_transform method learns the vocabulary and creates the document-term matrix (DTM) based on TF-IDF scores.\n",
    "vect_text_train = vect.fit_transform(dataset['Content'])\n",
    "vect_text_test = vect.fit_transform(test['Content'])\n",
    "vect_text_unseen = vect.fit_transform(unseen['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2t4zic8Ow5e",
    "outputId": "73b1038b-ba80-487c-b8a2-b51189681ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 415)\n",
      "  (0, 273)\t0.5257027241239838\n",
      "  (0, 341)\t0.5753214872470693\n",
      "  (0, 30)\t0.46319047555810816\n",
      "  (0, 13)\t0.4220147100667183\n",
      "  (1, 312)\t0.29939288139616543\n",
      "  (1, 36)\t0.2410407644883396\n",
      "  (1, 106)\t0.2735716583199712\n",
      "  (1, 29)\t0.4820815289766792\n",
      "  (1, 295)\t0.2410407644883396\n",
      "  (1, 365)\t0.1852883135094536\n",
      "  (1, 269)\t0.29939288139616543\n",
      "  (1, 370)\t0.29939288139616543\n",
      "  (1, 156)\t0.3937981841661617\n",
      "  (1, 334)\t0.20360876283851817\n",
      "  (1, 71)\t0.27709395035051004\n",
      "  (2, 2)\t0.17717522364710026\n",
      "  (2, 267)\t0.18509500681593224\n",
      "  (2, 209)\t0.24153829414113748\n",
      "  (2, 380)\t0.1944621221502927\n",
      "  (2, 92)\t0.1944621221502927\n",
      "  (2, 324)\t0.10782018152837647\n",
      "  (2, 73)\t0.17031479541542807\n",
      "  (2, 135)\t0.14948325745307753\n",
      "  (2, 326)\t0.22070675617878696\n",
      "  (2, 346)\t0.5552850204477967\n",
      "  :\t:\n",
      "  (76, 126)\t0.22029965658601144\n",
      "  (76, 358)\t0.2350524983635603\n",
      "  (76, 103)\t0.2437116325989612\n",
      "  (76, 16)\t0.22730663884221308\n",
      "  (76, 115)\t0.2782652038673873\n",
      "  (76, 362)\t0.26486134799384786\n",
      "  (76, 347)\t0.20256998430489428\n",
      "  (76, 71)\t0.15994308006029073\n",
      "  (77, 80)\t0.7071067811865476\n",
      "  (77, 378)\t0.7071067811865476\n",
      "  (78, 222)\t0.8268987565246838\n",
      "  (78, 94)\t0.56235082151441\n",
      "  (79, 178)\t0.29535412435828806\n",
      "  (79, 149)\t0.29535412435828806\n",
      "  (79, 368)\t0.29535412435828806\n",
      "  (79, 406)\t0.29535412435828806\n",
      "  (79, 350)\t0.29535412435828806\n",
      "  (79, 12)\t0.269881225016243\n",
      "  (79, 91)\t0.269881225016243\n",
      "  (79, 132)\t0.269881225016243\n",
      "  (79, 6)\t0.25180791530960295\n",
      "  (79, 239)\t0.269881225016243\n",
      "  (79, 38)\t0.269881225016243\n",
      "  (79, 129)\t0.25180791530960295\n",
      "  (79, 251)\t0.269881225016243\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of the TF-IDF matrix\n",
    "# This will show the dimensions of the matrix, where the first number is the number of documents and the second number is the number of features (terms).\n",
    "print(vect_text_unseen.shape)\n",
    "\n",
    "# Printing the TF-IDF matrix\n",
    "# This will display the sparse matrix representation of the TF-IDF features.\n",
    "# Each row corresponds to a document, and each column corresponds to a term, with values representing the TF-IDF scores.\n",
    "print(vect_text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Mb3LdI3MOzE1"
   },
   "outputs": [],
   "source": [
    "##Text Classification MODEL COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHf74aAePD9Y",
    "outputId": "e2ed8fb8-066e-484e-9bf9-3e97904d4245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5125\n",
      "Unseen Accuracy: 0.5333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.30      0.29      0.29        28\n",
      "    Positive       0.62      0.63      0.63        52\n",
      "\n",
      "    accuracy                           0.51        80\n",
      "   macro avg       0.46      0.46      0.46        80\n",
      "weighted avg       0.51      0.51      0.51        80\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.36      0.29      0.32        17\n",
      "    Positive       0.61      0.68      0.64        28\n",
      "\n",
      "    accuracy                           0.53        45\n",
      "   macro avg       0.49      0.49      0.48        45\n",
      "weighted avg       0.52      0.53      0.52        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training and evaluation of Model (Decision Tree)\n",
    "from sklearn import tree\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "tree.fit(vect_text_train,dataset['Sentiment'])\n",
    "\n",
    "y_pred = tree.predict(vect_text_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "y_pred_unseen = tree.predict(vect_text_unseen)\n",
    "accuracy_unseen = accuracy_score(y_unseen,y_pred_unseen)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Unseen Accuracy: {accuracy_unseen}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(classification_report(y_unseen, y_pred_unseen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_pf0oUeU0Yy",
    "outputId": "51d7a678-9fec-4418-b302-aa2eee4a6516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5\n",
      "Unseen Accuracy: 0.5777777777777777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.36      0.57      0.44        28\n",
      "    Positive       0.67      0.46      0.55        52\n",
      "\n",
      "    accuracy                           0.50        80\n",
      "   macro avg       0.52      0.52      0.49        80\n",
      "weighted avg       0.56      0.50      0.51        80\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.44      0.47      0.46        17\n",
      "    Positive       0.67      0.64      0.65        28\n",
      "\n",
      "    accuracy                           0.58        45\n",
      "   macro avg       0.56      0.56      0.56        45\n",
      "weighted avg       0.58      0.58      0.58        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training and evaluation of Model (Logistic Regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(random_state=0,penalty=None)\n",
    "LR.fit(vect_text_train,dataset['Sentiment'])\n",
    "\n",
    "y_pred = LR.predict(vect_text_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "y_pred_unseen = LR.predict(vect_text_unseen)\n",
    "accuracy_unseen = accuracy_score(y_unseen,y_pred_unseen)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Unseen Accuracy: {accuracy_unseen}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(classification_report(y_unseen, y_pred_unseen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJ8VSLSTXI6-",
    "outputId": "41c41586-a8b7-4691-88d4-cbf147f320c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5875\n",
      "Unseen Accuracy: 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.42      0.50      0.46        28\n",
      "    Positive       0.70      0.63      0.67        52\n",
      "\n",
      "    accuracy                           0.59        80\n",
      "   macro avg       0.56      0.57      0.56        80\n",
      "weighted avg       0.60      0.59      0.59        80\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.47      0.41      0.44        17\n",
      "    Positive       0.67      0.71      0.69        28\n",
      "\n",
      "    accuracy                           0.60        45\n",
      "   macro avg       0.57      0.56      0.56        45\n",
      "weighted avg       0.59      0.60      0.59        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(vect_text_train,dataset['Sentiment'])\n",
    "\n",
    "y_pred = rfc.predict(vect_text_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "y_pred_unseen = rfc.predict(vect_text_unseen)\n",
    "accuracy_unseen = accuracy_score(y_unseen,y_pred_unseen)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Unseen Accuracy: {accuracy_unseen}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(classification_report(y_unseen, y_pred_unseen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8h-KqAHsh3N"
   },
   "source": [
    "##Iterative Improvement of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "6eB1HoHKXrw5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc_grid = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 30, 50],\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf' : [1, 10, 20, 50],\n",
    "    'max_features': ['sqrt', 'log2',None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rfc_grid,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5, scoring='accuracy', verbose=3 ,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfI4RpAMuJmh",
    "outputId": "e8ae6545-85d5-4ceb-df3b-81012f98f991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4032 candidates, totalling 20160 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [None, 10, 30, 50],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 10, 20, 50],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10, 15],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150, 200, 300, 400, 500]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [None, 10, 30, 50],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 10, 20, 50],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10, 15],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150, 200, 300, 400, 500]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': [None, 10, 30, 50],\n",
       "                         'max_features': ['sqrt', 'log2', None],\n",
       "                         'min_samples_leaf': [1, 10, 20, 50],\n",
       "                         'min_samples_split': [2, 5, 10, 15],\n",
       "                         'n_estimators': [50, 100, 150, 200, 300, 400, 500]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(vect_text_train,dataset['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "3HEXEDaZuQsh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'log_loss', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Best Cross-Validation Score: 0.9350818260120587\n",
      "Test Accuracy: 0.5875\n",
      "Performance Metrics of Optimized Random Forest Classifier: \n",
      "\n",
      "Performance on test data:\n",
      "Accuracy on test data: 58.75%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.41      0.43      0.42        28\n",
      "    Positive       0.69      0.67      0.68        52\n",
      "\n",
      "    accuracy                           0.59        80\n",
      "   macro avg       0.55      0.55      0.55        80\n",
      "weighted avg       0.59      0.59      0.59        80\n",
      "\n",
      "\n",
      "Performance on unseen validation data:\n",
      "Accuracy on unseen data: 62.22%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      0.41      0.45        17\n",
      "    Positive       0.68      0.75      0.71        28\n",
      "\n",
      "    accuracy                           0.62        45\n",
      "   macro avg       0.59      0.58      0.58        45\n",
      "weighted avg       0.61      0.62      0.61        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and the corresponding score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = grid_search.best_estimator_.predict(vect_text_test)\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "# Evaluate Random Forest performance on the test set after GridSearchCV\n",
    "print(\"Performance Metrics of Optimized Random Forest Classifier: \")\n",
    "print(\"\\nPerformance on test data:\")\n",
    "print(f\"Accuracy on test data: {(accuracy_score(y_test, y_pred) * 100):.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate Random Forest performance on unseen split after GridSearchCV\n",
    "y_unseen_pred = grid_search.best_estimator_.predict(vect_text_unseen)\n",
    "print(\"\\nPerformance on unseen validation data:\")\n",
    "print(f\"Accuracy on unseen data: \" +\n",
    "      f\"{(accuracy_score(y_unseen, y_unseen_pred) * 100):.2f}%\")\n",
    "print(classification_report(y_unseen, y_unseen_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dont include in conclusion or whatnot this is for saving the model in case hanapin ni maam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('grid_model.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('grid_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'log_loss',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'log2',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
